% Encoding: UTF-8

@InProceedings{Liu2015,
  author    = {Liu, Mengyi and Li, Shaoxin and Shan, Shiguang and Wang, Ruiping and Chen, Xilin},
  title     = {Deeply Learning Deformable Facial Action Parts Model for Dynamic Expression Analysis},
  booktitle = {Computer Vision -- ACCV 2014},
  year      = {2015},
  editor    = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
  pages     = {143--157},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {Expressions are facial activities invoked by sets of muscle motions, which would give rise to large variations in appearance mainly around facial parts. Therefore, for visual-based expression analysis, localizing the action parts and encoding them effectively become two essential but challenging problems. To take them into account jointly for expression analysis, in this paper, we propose to adapt 3D Convolutional Neural Networks (3D CNN) with deformable action parts constraints. Specifically, we incorporate a deformable parts learning component into the 3D CNN framework, which can detect specific facial action parts under the structured spatial constraints, and obtain the discriminative part-based representation simultaneously. The proposed method is evaluated on two posed expression datasets, CK+, MMI, and a spontaneous dataset FERA. We show that, besides achieving state-of-the-art expression recognition accuracy, our method also enjoys the intuitive appeal that the part detection map can desirably encode the mid-level semantics of different facial action parts.},
  isbn      = {978-3-319-16817-3},
}

@Article{Marrero-Fernandez2019,
  author        = {Pedro D. Marrero{-}Fern{\'{a}}ndez and Fidel A. Guerrero{-}Pe{\~{n}}a and Tsang Ing Ren and Alexandre Cunha},
  title         = {FERAtt: Facial Expression Recognition with Attention Net},
  journal       = {CoRR},
  year          = {2019},
  volume        = {abs/1902.03284},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1902-03284},
  eprint        = {1902.03284},
  timestamp     = {Tue, 21 May 2019 18:03:40 +0200},
  url           = {http://arxiv.org/abs/1902.03284},
}

@Article{Liang2019,
  author   = {Liang, Dandan and Liang, Huagang and Yu, Zhenbo and Zhang, Yipu},
  title    = {Deep convolutional BiLSTM fusion network for facial expression recognition},
  journal  = {The Visual Computer},
  year     = {2019},
  month    = {Feb},
  issn     = {1432-2315},
  abstract = {Deep learning algorithms have shown significant performance improvements for facial expression recognition (FER). Most deep learning-based methods, however, focus more attention on spatial appearance features for classification, discarding much useful temporal information. In this work, we present a novel framework that jointly learns spatial features and temporal dynamics for FER. Given the image sequence of an expression, spatial features are extracted from each frame using a deep network, while the temporal dynamics are modeled by a convolutional network, which takes a pair of consecutive frames as input. Finally, the framework accumulates clues from fused features by a BiLSTM network. In addition, the framework is end-to-end learnable, and thus temporal information can be adapted to complement spatial features. Experimental results on three benchmark databases, CK+, Oulu-CASIA and MMI, show that the proposed framework outperforms state-of-the-art methods.},
  day      = {01},
  doi      = {10.1007/s00371-019-01636-3},
  url      = {https://doi.org/10.1007/s00371-019-01636-3},
}

@Article{Kim2017,
  author        = {Youngsung Kim and ByungIn Yoo and Youngjun Kwak and Changkyu Choi and Junmo Kim},
  title         = {Deep generative-contrastive networks for facial expression recognition},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1703.07140},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/KimYKCK17},
  eprint        = {1703.07140},
  timestamp     = {Mon, 13 Aug 2018 16:48:22 +0200},
  url           = {http://arxiv.org/abs/1703.07140},
}

@Article{Makhmudkhujaev2019,
  author   = {Farkhod Makhmudkhujaev and M. Abdullah-Al-Wadud and Md Tauhid Bin Iqbal and Byungyong Ryu and Oksam Chae},
  title    = {Facial expression recognition with local prominent directional pattern},
  journal  = {Signal Processing: Image Communication},
  year     = {2019},
  volume   = {74},
  pages    = {1 - 12},
  issn     = {0923-5965},
  abstract = {Local edge-based descriptors have gained much attention as feature extraction methods for facial expression recognition. However, such descriptors are found to suffer from unstable shape representations for different local structures for their sensitivity to local distortions such as noise and positional variations. We propose a novel edge-based descriptor, named Local Prominent Directional Pattern (LPDP), which considers statistical information of a pixel neighborhood to encode more meaningful and reliable information than the existing descriptors for feature extraction. More specifically, LPDP examines a local neighborhood of a pixel to retrieve significant edges corresponding to the local shape and thereby ensures encoding edge information in spite of some positional variations and avoiding noisy edges. Thus LPDP can represent important textured regions much effectively to be used in facial expression recognition. Extensive experiments on facial expression recognition on well-known datasets also demonstrate the better capability of LPDP than other existing descriptors in terms of robustness in extracting various local structures originated by facial expression changes.},
  doi      = {https://doi.org/10.1016/j.image.2019.01.002},
  keywords = {LDPD, Histogram of directional variations, Prominent directions, Shape pattern, Facial expression recognition},
  url      = {http://www.sciencedirect.com/science/article/pii/S0923596518306556},
}

@Article{Zhang2019,
  author   = {S. {Zhang} and X. {Pan} and Y. {Cui} and X. {Zhao} and L. {Liu}},
  title    = {Learning Affective Video Features for Facial Expression Recognition via Hybrid Deep Learning},
  journal  = {IEEE Access},
  year     = {2019},
  volume   = {7},
  pages    = {32297-32304},
  issn     = {2169-3536},
  abstract = {One key challenging issues of facial expression recognition (FER) in video sequences is to extract discriminative spatiotemporal video features from facial expression images in video sequences. In this paper, we propose a new method of FER in video sequences via a hybrid deep learning model. The proposed method first employs two individual deep convolutional neural networks (CNNs), including a spatial CNN processing static facial images and a temporal CN network processing optical flow images, to separately learn high-level spatial and temporal features on the divided video segments. These two CNNs are fine-tuned on target video facial expression datasets from a pre-trained CNN model. Then, the obtained segment-level spatial and temporal features are integrated into a deep fusion network built with a deep belief network (DBN) model. This deep fusion network is used to jointly learn discriminative spatiotemporal features. Finally, an average pooling is performed on the learned DBN segment-level features in a video sequence, to produce a fixed-length global video feature representation. Based on the global video feature representations, a linear support vector machine (SVM) is employed for facial expression classification tasks. The extensive experiments on three public video-based facial expression datasets, i.e., BAUM-1s, RML, and MMI, show the effectiveness of our proposed method, outperforming the state-of-the-arts.},
  doi      = {10.1109/ACCESS.2019.2901521},
  keywords = {belief networks;convolutional neural nets;emotion recognition;face recognition;feature extraction;image classification;image representation;image sequences;learning (artificial intelligence);support vector machines;video signal processing;deep convolutional neural networks;optical flow images;video segments;public video-based facial expression datasets;facial expression classification tasks;fixed-length global video feature representation;learned DBN segment-level features;deep belief network model;deep fusion network;pre-trained CNN model;hybrid deep learning model;facial expression images;discriminative spatiotemporal video features;video sequence;facial expression recognition;affective video features;Video sequences;Feature extraction;Deep learning;Optical flow;Face recognition;Image segmentation;Task analysis;Facial expression recognition;spatio-temporal features;hybrid deep learning;deep convolutional neural networks;deep belief network},
}

@InProceedings{Acharya2018,
  author    = {D. {Acharya} and Z. {Huang} and D. P. {Paudel} and L. {Van Gool}},
  title     = {Covariance Pooling for Facial Expression Recognition},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year      = {2018},
  pages     = {480-4807},
  month     = {June},
  abstract  = {Classifying facial expressions into different categories requires capturing regional distortions of facial landmarks. We believe that second-order statistics such as covariance is better able to capture such distortions in regional facial features. In this work, we explore the benefits of using a manifold network structure for covariance pooling to improve facial expression recognition. In particular, we first employ such kind of manifold networks in conjunction with traditional convolutional networks for spatial pooling within individual image feature maps in an end-to-end deep learning manner. By doing so, we are able to achieve a recognition accuracy of 58.14% on the validation set of Static Facial Expressions in the Wild (SFEW2.0) and 87.0% on the validation set of Real-World Affective Faces (RAF) Database1. Both of these results are the best results we are aware of. Besides, we leverage covariance pooling to capture the temporal evolution of per-frame features for video-based facial expression recognition. Our reported results demonstrate the advantage of pooling image-set features temporally by stacking the designed manifold network of covariance pooling on top of convolutional network layers.},
  doi       = {10.1109/CVPRW.2018.00077},
  issn      = {2160-7516},
  keywords  = {convolutional neural nets;emotion recognition;face recognition;feature extraction;image classification;learning (artificial intelligence);manifold networks;spatial pooling;individual image feature maps;end-to-end deep learning manner;recognition accuracy;leverage covariance pooling;video-based facial expression recognition;image-set features;convolutional network layers;regional distortions;facial landmarks;regional facial features;manifold network structure;static facial expressions;Face recognition;Covariance matrices;Manifolds;Image recognition;Videos;Feature extraction;Standards},
}

@InProceedings{Kuo2018,
  author    = {Kuo, Chieh-Ming and Lai, Shang-Hong and Sarkis, Michel},
  title     = {A Compact Deep Learning Model for Robust Facial Expression Recognition},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year      = {2018},
  month     = {June},
}

@Article{Zhang2019a,
  author   = {T. {Zhang} and W. {Zheng} and Z. {Cui} and Y. {Zong} and Y. {Li}},
  title    = {Spatial–Temporal Recurrent Neural Network for Emotion Recognition},
  journal  = {IEEE Transactions on Cybernetics},
  year     = {2019},
  volume   = {49},
  number   = {3},
  pages    = {839-847},
  month    = {March},
  issn     = {2168-2267},
  abstract = {In this paper, we propose a novel deep learning framework, called spatial-temporal recurrent neural network (STRNN), to integrate the feature learning from both spatial and temporal information of signal sources into a unified spatial-temporal dependency model. In STRNN, to capture those spatially co-occurrent variations of human emotions, a multidirectional recurrent neural network (RNN) layer is employed to capture long-range contextual cues by traversing the spatial regions of each temporal slice along different directions. Then a bi-directional temporal RNN layer is further used to learn the discriminative features characterizing the temporal dependencies of the sequences, where sequences are produced from the spatial RNN layer. To further select those salient regions with more discriminative ability for emotion recognition, we impose sparse projection onto those hidden states of spatial and temporal domains to improve the model discriminant ability. Consequently, the proposed two-layer RNN model provides an effective way to make use of both spatial and temporal dependencies of the input signals for emotion recognition. Experimental results on the public emotion datasets of electroencephalogram and facial expression demonstrate the proposed STRNN method is more competitive over those state-of-the-art methods.},
  doi      = {10.1109/TCYB.2017.2788081},
  keywords = {electroencephalography;emotion recognition;face recognition;feature extraction;learning (artificial intelligence);neural nets;recurrent neural nets;feature learning;spatial information;temporal information;spatial-temporal dependency model;human emotions;multidirectional recurrent neural network layer;temporal slice;temporal dependencies;spatial RNN layer;emotion recognition;spatial domains;temporal domains;two-layer RNN model;deep learning framework;spatial-temporal recurrent neural network;bidirectional temporal RNN layer;Emotion recognition;Brain modeling;Electroencephalography;Recurrent neural networks;Electrodes;Face recognition;Cybernetics;Electroencephalogram (EEG) emotion recognition;emotion recognition;facial expression recognition;spatial–temporal recurrent neural network (STRNN)},
}

@Article{Chen2018,
  author   = {J. {Chen} and Z. {Chen} and Z. {Chi} and H. {Fu}},
  title    = {Facial Expression Recognition in Video with Multiple Feature Fusion},
  journal  = {IEEE Transactions on Affective Computing},
  year     = {2018},
  volume   = {9},
  number   = {1},
  pages    = {38-50},
  month    = {Jan},
  issn     = {1949-3045},
  abstract = {Video based facial expression recognition has been a long standing problem and attracted growing attention recently. The key to a successful facial expression recognition system is to exploit the potentials of audiovisual modalities and design robust features to effectively characterize the facial appearance and configuration changes caused by facial motions. We propose an effective framework to address this issue in this paper. In our study, both visual modalities (face images) and audio modalities (speech) are utilized. A new feature descriptor called Histogram of Oriented Gradients from Three Orthogonal Planes (HOG-TOP) is proposed to extract dynamic textures from video sequences to characterize facial appearance changes. And a new effective geometric feature derived from the warp transformation of facial landmarks is proposed to capture facial configuration changes. Moreover, the role of audio modalities on recognition is also explored in our study. We applied the multiple feature fusion to tackle the video-based facial expression recognition problems under lab-controlled environment and in the wild, respectively. Experiments conducted on the extended Cohn-Kanade (CK+) database and the Acted Facial Expression in Wild (AFEW) 4.0 database show that our approach is robust in dealing with video-based facial expression recognition problems under lab-controlled environment and in the wild compared with the other state-of-the-art methods.},
  doi      = {10.1109/TAFFC.2016.2593719},
  keywords = {emotion recognition;face recognition;feature extraction;image texture;visual databases;facial motions;visual modalities;audio modalities;feature descriptor;video sequences;facial appearance changes;facial landmarks;facial configuration changes;multiple feature fusion;video based facial expression recognition;audiovisual modalities;design robust features;geometric feature;Acted Facial Expression in Wild;dynamic texture extraction;Histogram of Oriented Gradients from Three Orthogonal Planes;HOG-TOP;AFEW;video-based facial expression recognition problems;Face recognition;Feature extraction;Visualization;Histograms;Video sequences;Acoustics;Facial expression recognition;multiple feature fusion;HOG-TOP;geometric warp feature;acoustic feature},
}

@Article{Yan2018,
  author   = {Haibin Yan},
  title    = {Collaborative discriminative multi-metric learning for facial expression recognition in video},
  journal  = {Pattern Recognition},
  year     = {2018},
  volume   = {75},
  pages    = {33 - 40},
  issn     = {0031-3203},
  note     = {Distance Metric Learning for Pattern Recognition},
  abstract = {Facial expression recognition in video has been an important and relatively new topic in human face analysis and attracted growing interests in recent years. Unlike conventional image-based facial expression recognition methods which recognize facial expression category from still images, facial expression recognition in video is more challenging because there are usually larger intra-class variations among facial frames within a video. This paper presents a collaborative discriminative multi-metric learning (CDMML) for facial expression recognition in video. We first compute multiple feature descriptors for each face video to describe facial appearance and motion information from different aspects. Then, we learn multiple distance metrics with these extracted multiple features collaboratively to exploit complementary and discriminative information for recognition. Experimental results on the Acted Facial Expression in Wild (AFEW) 4.0 and the extended Cohn–Kanada (CK+) datasets are presented to demonstrate the effectiveness of our proposed method.},
  doi      = {https://doi.org/10.1016/j.patcog.2017.02.031},
  keywords = {Facial expression recognition, Multi-metric learning, Collaborative learning, Video-based, Multi-view learning},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320317300948},
}

@Article{Zhang2017,
  author   = {K. {Zhang} and Y. {Huang} and Y. {Du} and L. {Wang}},
  title    = {Facial Expression Recognition Based on Deep Evolutional Spatial-Temporal Networks},
  journal  = {IEEE Transactions on Image Processing},
  year     = {2017},
  volume   = {26},
  number   = {9},
  pages    = {4193-4203},
  month    = {Sep.},
  issn     = {1057-7149},
  abstract = {One key challenging issue of facial expression recognition is to capture the dynamic variation of facial physical structure from videos. In this paper, we propose a part-based hierarchical bidirectional recurrent neural network (PHRNN) to analyze the facial expression information of temporal sequences. Our PHRNN models facial morphological variations and dynamical evolution of expressions, which is effective to extract “temporal features” based on facial landmarks (geometry information) from consecutive frames. Meanwhile, in order to complement the still appearance information, a multi-signal convolutional neural network (MSCNN) is proposed to extract “spatial features” from still frames. We use both recognition and verification signals as supervision to calculate different loss functions, which are helpful to increase the variations of different expressions and reduce the differences among identical expressions. This deep evolutional spatial-temporal network (composed of PHRNN and MSCNN) extracts the partial-whole, geometry-appearance, and dynamic-still information, effectively boosting the performance of facial expression recognition. Experimental results show that this method largely outperforms the state-of-the-art ones. On three widely used facial expression databases (CK+, Oulu-CASIA, and MMI), our method reduces the error rates of the previous best ones by 45.5%, 25.8%, and 24.4%, respectively.},
  doi      = {10.1109/TIP.2017.2689999},
  keywords = {emotion recognition;face recognition;geometry;recurrent neural nets;video signal processing;deep evolutional spatial-temporal networks;facial physical structure;videos;part-based hierarchical bidirectional recurrent neural network;PHRNN;facial expression information analysis;temporal sequences;facial morphological variations;expression dynamical evolution;temporal feature extraction;facial landmarks;geometry information;multisignal convolutional neural network;MSCNN;spatial feature extraction;verification signals;loss function calculation;partial-whole information extraction;geometry-appearance information extraction;dynamic-still information extraction;Feature extraction;Face recognition;Databases;Recurrent neural networks;Data mining;Image recognition;Facial expression recognition;dynamical evolution;recognition and verification signals;deep spatial-temporal networks},
}

@InProceedings{Hasani2017,
  author    = {Hasani; Mohammad Mahoor, Behzad H.},
  title     = {Facial Expression Recognition Using Enhanced Deep 3D Convolutional Neural Networks},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year      = {2017},
  month     = {July},
}

@InProceedings{Hasani2017a,
  author    = {B. {Hasani} and M. H. {Mahoor}},
  title     = {Spatio-Temporal Facial Expression Recognition Using Convolutional Neural Networks and Conditional Random Fields},
  booktitle = {2017 12th IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)},
  year      = {2017},
  pages     = {790-795},
  month     = {May},
  abstract  = {Automated Facial Expression Recognition (FER) has been a challenging task for decades. Many of the existing works use hand-crafted features such as LBP, HOG, LPQ, and Histogram of Optical Flow (HOF) combined with classifiers such as Support Vector Machines for expression recognition. These methods often require rigorous hyperparameter tuning to achieve good results. Recently Deep Neural Networks (DNN) have shown to outperform traditional methods in visual object recognition. In this paper, we propose a two-part network consisting of a DNN-based architecture followed by a Conditional Random Field (CRF) module for facial expression recognition in videos. The first part captures the spatial relation within facial images using convolutional layers followed by three Inception- ResNet modules and two fully-connected layers. To capture the temporal relation between the image frames, we use linear chain CRF in the second part of our network. We evaluate our proposed network on three publicly available databases, viz. CK+, MMI, and FERA. Experiments are performed in subjectindependent and cross-database manners. Our experimental results show that cascading the deep network architecture with the CRF module considerably increases the recognition of facial expressions in videos and in particular it outperforms the stateof- the-art methods in the cross-database experiments and yields comparable results in the subject-independent experiments.},
  doi       = {10.1109/FG.2017.99},
  keywords  = {emotion recognition;face recognition;neural nets;object recognition;video signal processing;spatio-temporal facial expression recognition;convolutional neural networks;conditional random fields;CRF module;automated facial expression recognition;automated FER;hand-crafted features;hyperparameter tuning;deep neural networks;DNN;visual object recognition;videos;facial images;convolutional layers;Inception-ResNet modules;image frame temporal relation;linear chain CRF;CK+;MMI;FERA;cross-database manners;deep network architecture cascading;cross-database experiments;subject-independent experiments;Face recognition;Training;Hidden Markov models;Databases;Feature extraction;Computer architecture;Neural networks},
}

@InProceedings{Ding2017,
  author    = {H. {Ding} and S. K. {Zhou} and R. {Chellappa}},
  title     = {FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition},
  booktitle = {2017 12th IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)},
  year      = {2017},
  pages     = {118-126},
  month     = {May},
  abstract  = {Relatively small data sets available for expression recognition research make the training of deep networks very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redundant information from the pretrained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully-connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization results show that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu- CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.},
  doi       = {10.1109/FG.2017.23},
  keywords  = {convolution;data visualisation;emotion recognition;face recognition;learning (artificial intelligence);FaceNet2ExpNet;deep face recognition net;expression recognition;deep network training;static images;convolutional layer training;visualization;CK+;Oulu- CASIA;TFD;SFEW;deep convolutional neural networks;Face;Training;Face recognition;Neurons;Image recognition;Convolution;Distribution functions},
}

@Article{Lopes2017,
  author   = {André Teixeira Lopes and Edilson de Aguiar and Alberto F. De Souza and Thiago Oliveira-Santos},
  title    = {Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order},
  journal  = {Pattern Recognition},
  year     = {2017},
  volume   = {61},
  pages    = {610 - 628},
  issn     = {0031-3203},
  abstract = {Facial expression recognition has been an active research area in the past 10 years, with growing application areas including avatar animation, neuromarketing and sociable robots. The recognition of facial expressions is not an easy problem for machine learning methods, since people can vary significantly in the way they show their expressions. Even images of the same person in the same facial expression can vary in brightness, background and pose, and these variations are emphasized if considering different subjects (because of variations in shape, ethnicity among others). Although facial expression recognition is very studied in the literature, few works perform fair evaluation avoiding mixing subjects while training and testing the proposed algorithms. Hence, facial expression recognition is still a challenging problem in computer vision. In this work, we propose a simple solution for facial expression recognition that uses a combination of Convolutional Neural Network and specific image pre-processing steps. Convolutional Neural Networks achieve better accuracy with big data. However, there are no publicly available datasets with sufficient data for facial expression recognition with deep architectures. Therefore, to tackle the problem, we apply some pre-processing techniques to extract only expression specific features from a face image and explore the presentation order of the samples during training. The experiments employed to evaluate our technique were carried out using three largely used public databases (CK+, JAFFE and BU-3DFE). A study of the impact of each image pre-processing operation in the accuracy rate is presented. The proposed method: achieves competitive results when compared with other facial expression recognition methods – 96.76% of accuracy in the CK+ database – it is fast to train, and it allows for real time facial expression recognition with standard computers.},
  doi      = {https://doi.org/10.1016/j.patcog.2016.07.026},
  keywords = {Facial expression recognition, Convolutional Neural Networks, Computer vision, Machine learning, Expression specific features},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320316301753},
}

@Article{Ryu2017,
  author   = {B. {Ryu} and A. R. {Rivera} and J. {Kim} and O. {Chae}},
  title    = {Local Directional Ternary Pattern for Facial Expression Recognition},
  journal  = {IEEE Transactions on Image Processing},
  year     = {2017},
  volume   = {26},
  number   = {12},
  pages    = {6006-6018},
  month    = {Dec},
  issn     = {1057-7149},
  abstract = {This paper presents a new face descriptor, local directional ternary pattern (LDTP), for facial expression recognition. LDTP efficiently encodes information of emotion-related features (ı.e., eyes, eyebrows, upper nose, and mouth) by using the directional information and ternary pattern in order to take advantage of the robustness of edge patterns in the edge region while overcoming weaknesses of edge-based methods in smooth regions. Our proposal, unlike existing histogram-based face description methods that divide the face into several regions and sample the codes uniformly, uses a two-level grid to construct the face descriptor while sampling expression-related information at different scales. We use a coarse grid for stable codes (highly related to non-expression), and a finer one for active codes (highly related to expression). This multi-level approach enables us to do a finer grain description of facial motions while still characterizing the coarse features of the expression. Moreover, we learn the active LDTP codes from the emotion-related facial regions. We tested our method by using person-dependent and independent cross-validation schemes to evaluate the performance. We show that our approaches improve the overall accuracy of facial expression recognition on six data sets.},
  doi      = {10.1109/TIP.2017.2726010},
  keywords = {emotion recognition;face recognition;local directional ternary pattern;facial expression recognition;LDTP;emotion-related features;two-level grid;face descriptor;active codes;stable codes;person-dependent scheme;independent cross-validation schemes;Face recognition;Face;Image edge detection;Histograms;Compass;Feature extraction;Emotion recognition;Face descriptor;local pattern;expression recognition;edge pattern;face recognition},
}

@InProceedings{Mollahosseini2016,
  author    = {A. {Mollahosseini} and D. {Chan} and M. H. {Mahoor}},
  title     = {Going deeper in facial expression recognition using deep neural networks},
  booktitle = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year      = {2016},
  pages     = {1-10},
  month     = {March},
  abstract  = {Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem in computer vision. Despite efforts made in developing various methods for FER, existing approaches lack generalizability when applied to unseen images or those that are captured in wild setting (i.e. the results are not significant). Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyper-parameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publicly available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of our proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks in both accuracy and training time.},
  doi       = {10.1109/WACV.2016.7477450},
  keywords  = {computer vision;convolution;face recognition;neural net architecture;visual databases;FER;computer vision;engineered features;recognition accuracies;deep neural network architecture;standard face datasets;convolutional layers;max pooling;inception layers;single component architecture;registered facial images;neutral expressions;facial expression databases;MultiPIE;MMI;CK+;DISFA;FERA;SFEW;FER2013;convolutional neural networks;training time;automated facial expression recognition;Databases;Computer architecture;Face;Training;Feature extraction;Biological neural networks},
}

@InProceedings{Zhao2016,
  author    = {Zhao, Xiangyun and Liang, Xiaodan and Liu, Luoqi and Li, Teng and Han, Yugang and Vasconcelos, Nuno and Yan, Shuicheng},
  title     = {Peak-Piloted Deep Network for Facial Expression Recognition},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  editor    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  pages     = {425--442},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A special-purpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-of-the-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper definition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset.},
  isbn      = {978-3-319-46475-6},
}

@InProceedings{Jung2015,
  author    = {H. {Jung} and S. {Lee} and J. {Yim} and S. {Park} and J. {Kim}},
  title     = {Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  year      = {2015},
  pages     = {2983-2991},
  month     = {Dec},
  abstract  = {Temporal information has useful features for recognizing facial expressions. However, to manually design useful features requires a lot of effort. In this paper, to reduce this effort, a deep learning technique, which is regarded as a tool to automatically extract useful features from raw data, is adopted. Our deep network is based on two different models. The first deep network extracts temporal appearance features from image sequences, while the other deep network extracts temporal geometry features from temporal facial landmark points. These two models are combined using a new integration method in order to boost the performance of the facial expression recognition. Through several experiments, we show that the two models cooperate with each other. As a result, we achieve superior performance to other state-of-the-art methods in the CK+ and Oulu-CASIA databases. Furthermore, we show that our new integration method gives more accurate results than traditional methods, such as a weighted summation and a feature concatenation method.},
  doi       = {10.1109/ICCV.2015.341},
  issn      = {2380-7504},
  keywords  = {computational geometry;face recognition;feature extraction;image sequences;learning (artificial intelligence);neural nets;joint fine-tuning;deep neural networks;facial expression recognition;temporal information;image sequences;weighted summation;feature concatenation method;Image sequences;Three-dimensional displays;Face recognition;Feature extraction;Databases;Image recognition;Training},
}

@Article{Fan2015,
  author   = {Xijian Fan and Tardi Tjahjadi},
  title    = {A spatial-temporal framework based on histogram of gradients and optical flow for facial expression recognition in video sequences},
  journal  = {Pattern Recognition},
  year     = {2015},
  volume   = {48},
  number   = {11},
  pages    = {3407 - 3416},
  issn     = {0031-3203},
  abstract = {Facial expression causes different parts of the facial region to change over time and thus dynamic descriptors are inherently more suitable than static descriptors for recognising facial expressions. In this paper, we extend the spatial pyramid histogram of gradients to spatio-temporal domain to give 3-dimensional facial features and integrate them with dense optical flow to give a spatio-temporal descriptor which extracts both the spatial and dynamic motion information of facial expressions. A multi-class support vector machine based classifier with one-to-one strategy is used to recognise facial expressions. Experiments on the CK+ and MMI datasets using leave-one-out cross validation scheme demonstrate that the integrated framework achieves a better performance than using individual descriptor separately. Compared with six state of the art methods, the proposed framework demonstrates a superior performance.},
  doi      = {https://doi.org/10.1016/j.patcog.2015.04.025},
  keywords = {Histogram of gradients, Facial expression, Optical flow, Feature extraction},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320315001648},
}

@Article{Kaya2017,
  author   = {Heysem Kaya and Furkan Gürpınar and Albert Ali Salah},
  title    = {Video-based emotion recognition in the wild using deep transfer learning and score fusion},
  journal  = {Image and Vision Computing},
  year     = {2017},
  volume   = {65},
  pages    = {66 - 75},
  issn     = {0262-8856},
  note     = {Multimodal Sentiment Analysis and Mining in the Wild Image and Vision Computing},
  abstract = {Multimodal recognition of affective states is a difficult problem, unless the recording conditions are carefully controlled. For recognition “in the wild”, large variances in face pose and illumination, cluttered backgrounds, occlusions, audio and video noise, as well as issues with subtle cues of expression are some of the issues to target. In this paper, we describe a multimodal approach for video-based emotion recognition in the wild. We propose using summarizing functionals of complementary visual descriptors for video modeling. These features include deep convolutional neural network (CNN) based features obtained via transfer learning, for which we illustrate the importance of flexible registration and fine-tuning. Our approach combines audio and visual features with least squares regression based classifiers and weighted score level fusion. We report state-of-the-art results on the EmotiW Challenge for “in the wild” facial expression recognition. Our approach scales to other problems, and ranked top in the ChaLearn-LAP First Impressions Challenge 2016 from video clips collected in the wild.},
  doi      = {https://doi.org/10.1016/j.imavis.2017.01.012},
  keywords = {EmotiW, Emotion recognition in the wild, Multimodal fusion, Convolutional neural networks, Kernel extreme learning machine, Partial least squares},
  url      = {http://www.sciencedirect.com/science/article/pii/S0262885617300367},
}

@Comment{jabref-meta: databaseType:bibtex;}
